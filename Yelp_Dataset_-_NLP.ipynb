{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Data Challenge - NLP\n",
    "\n",
    "BitTiger DS501-1802\n",
    "\n",
    "May 2018\n",
    "\n",
    "Yan Wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/last_2_years_restaurant_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>categories</th>\n",
       "      <th>avg_stars</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "      <th>date_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--9e1ONYQuAa-CB_Rrw7Tw</td>\n",
       "      <td>Delmonico Steakhouse</td>\n",
       "      <td>['Cajun/Creole', 'Steakhouses', 'Restaurants']</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-03-31</td>\n",
       "      <td>0</td>\n",
       "      <td>6SgvNWJltnZhW7duJgZ42w</td>\n",
       "      <td>5</td>\n",
       "      <td>This is mine and my fiancé's favorite steakhou...</td>\n",
       "      <td>0</td>\n",
       "      <td>oFyOUOeGTRZhFPF9uTqrTQ</td>\n",
       "      <td>2016-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--9e1ONYQuAa-CB_Rrw7Tw</td>\n",
       "      <td>Delmonico Steakhouse</td>\n",
       "      <td>['Cajun/Creole', 'Steakhouses', 'Restaurants']</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-06-29</td>\n",
       "      <td>0</td>\n",
       "      <td>iwx6s6yQxc7yjS7NFANZig</td>\n",
       "      <td>4</td>\n",
       "      <td>Nice atmosphere and wonderful service. I had t...</td>\n",
       "      <td>0</td>\n",
       "      <td>2aeNFntqY2QDZLADNo8iQQ</td>\n",
       "      <td>2015-06-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--9e1ONYQuAa-CB_Rrw7Tw</td>\n",
       "      <td>Delmonico Steakhouse</td>\n",
       "      <td>['Cajun/Creole', 'Steakhouses', 'Restaurants']</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-03-16</td>\n",
       "      <td>0</td>\n",
       "      <td>UVUMu_bELdA56Ryfbur-DA</td>\n",
       "      <td>5</td>\n",
       "      <td>Every year a group of us (we had 6 this year) ...</td>\n",
       "      <td>1</td>\n",
       "      <td>gmPP4YFrgYsYQqPYokMgFA</td>\n",
       "      <td>2015-03-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>--9e1ONYQuAa-CB_Rrw7Tw</td>\n",
       "      <td>Delmonico Steakhouse</td>\n",
       "      <td>['Cajun/Creole', 'Steakhouses', 'Restaurants']</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-02-10</td>\n",
       "      <td>0</td>\n",
       "      <td>UxFpgng8dPMWOj99653k5Q</td>\n",
       "      <td>5</td>\n",
       "      <td>Truly Fantastic!  Best Steak ever. Service was...</td>\n",
       "      <td>0</td>\n",
       "      <td>aVOGlN9fZ-BXcbtj6dbf0g</td>\n",
       "      <td>2016-02-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>--9e1ONYQuAa-CB_Rrw7Tw</td>\n",
       "      <td>Delmonico Steakhouse</td>\n",
       "      <td>['Cajun/Creole', 'Steakhouses', 'Restaurants']</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-02-14</td>\n",
       "      <td>0</td>\n",
       "      <td>Xp3ppynEvVu1KxDHQ3ae8w</td>\n",
       "      <td>5</td>\n",
       "      <td>Delmonico Steakhouse is a steakhouse owned by ...</td>\n",
       "      <td>0</td>\n",
       "      <td>KC8H7qTZVPIEnanw9fG43g</td>\n",
       "      <td>2017-02-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                  name  \\\n",
       "0  --9e1ONYQuAa-CB_Rrw7Tw  Delmonico Steakhouse   \n",
       "1  --9e1ONYQuAa-CB_Rrw7Tw  Delmonico Steakhouse   \n",
       "2  --9e1ONYQuAa-CB_Rrw7Tw  Delmonico Steakhouse   \n",
       "3  --9e1ONYQuAa-CB_Rrw7Tw  Delmonico Steakhouse   \n",
       "4  --9e1ONYQuAa-CB_Rrw7Tw  Delmonico Steakhouse   \n",
       "\n",
       "                                       categories  avg_stars  cool  \\\n",
       "0  ['Cajun/Creole', 'Steakhouses', 'Restaurants']        4.0     0   \n",
       "1  ['Cajun/Creole', 'Steakhouses', 'Restaurants']        4.0     0   \n",
       "2  ['Cajun/Creole', 'Steakhouses', 'Restaurants']        4.0     0   \n",
       "3  ['Cajun/Creole', 'Steakhouses', 'Restaurants']        4.0     0   \n",
       "4  ['Cajun/Creole', 'Steakhouses', 'Restaurants']        4.0     0   \n",
       "\n",
       "         date  funny               review_id  stars  \\\n",
       "0  2016-03-31      0  6SgvNWJltnZhW7duJgZ42w      5   \n",
       "1  2015-06-29      0  iwx6s6yQxc7yjS7NFANZig      4   \n",
       "2  2015-03-16      0  UVUMu_bELdA56Ryfbur-DA      5   \n",
       "3  2016-02-10      0  UxFpgng8dPMWOj99653k5Q      5   \n",
       "4  2017-02-14      0  Xp3ppynEvVu1KxDHQ3ae8w      5   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  This is mine and my fiancé's favorite steakhou...       0   \n",
       "1  Nice atmosphere and wonderful service. I had t...       0   \n",
       "2  Every year a group of us (we had 6 this year) ...       1   \n",
       "3  Truly Fantastic!  Best Steak ever. Service was...       0   \n",
       "4  Delmonico Steakhouse is a steakhouse owned by ...       0   \n",
       "\n",
       "                  user_id    date_new  \n",
       "0  oFyOUOeGTRZhFPF9uTqrTQ  2016-03-31  \n",
       "1  2aeNFntqY2QDZLADNo8iQQ  2015-06-29  \n",
       "2  gmPP4YFrgYsYQqPYokMgFA  2015-03-16  \n",
       "3  aVOGlN9fZ-BXcbtj6dbf0g  2016-02-10  \n",
       "4  KC8H7qTZVPIEnanw9fG43g  2017-02-14  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your feature variables, here is the text of the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take the values of the column that contains review text data, save to a variable named \"documents\".\n",
    "documents = df['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((515752,), dtype('O'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect your documents, e.g. check the size, take a peek at elements of the numpy array\n",
    "documents.shape, documents.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ \"This is mine and my fiancé's favorite steakhouse here in Las Vegas! We often stop in and see our favorite server, Michael Jackson, who simply completes the fine dining experience for us. He is always so pleasant and knowledgable about the menu and suggests amazing dishes each time! I have never been disappointed here and will be back soon!\",\n",
       "       \"Nice atmosphere and wonderful service. I had the dinner special which was a wedge salad, a petite ribeye, and desert.   \\n\\nThe salad was as expected. Nothing to jump up and down about. The petite ribeye was tasty. Just a minute over cooked, but overall it was good. The desert was ice cream and cake. As I'm not a cake eater I can't tell you about that, but the ice cream was good. \\n\\nOverall, it was a pleasant dinner.\"], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your target variable (any categorical variable that may be meaningful)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For example, I am interested in perfect (5 stars) and imperfect (1-4 stars) rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a column and take the values, save to a variable named \"target\"\n",
    "target = []\n",
    "for i in range(df.shape[0]):\n",
    "    if df.loc[i, 'stars'] == 5:\n",
    "        rating = 1\n",
    "    else:\n",
    "        rating = 0\n",
    "    target.append(rating)\n",
    "target = np.asarray(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((515752,), dtype('int64'))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape, target.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You may want to look at the statistic of the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46397299477268145"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To be implemented, 46.4% of reviews give 5 stars.\n",
    "target.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create training dataset and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Documents is your X, target is your y\n",
    "# Now split the data to training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split to documents_train, documents_test, target_train, target_test\n",
    "documents_train, documents_test, target_train, target_test = train_test_split(documents, target, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get NLP representation of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create TfidfVectorizer, and name it vectorizer\n",
    "from nltk.corpus import stopwords\n",
    "vectorizer = TfidfVectorizer(stop_words = stopwords.words('english'), max_features = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the model with your training data\n",
    "vectors = vectorizer.fit_transform(documents_train).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the vocab of your tfidf\n",
    "words = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the trained model to transform your test data\n",
    "X_test = vectorizer.transform(documents_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(361026, 2000)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar review search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# We will need these helper methods pretty soon\n",
    "\n",
    "def get_top_values(lst, n, labels):\n",
    "    '''\n",
    "    INPUT: LIST, INTEGER, LIST\n",
    "    OUTPUT: LIST\n",
    "\n",
    "    Given a list of values, find the indices with the highest n values.\n",
    "    Return the labels for each of these indices.\n",
    "\n",
    "    e.g.\n",
    "    lst = [7, 3, 2, 4, 1]\n",
    "    n = 2\n",
    "    labels = [\"cat\", \"dog\", \"mouse\", \"pig\", \"rabbit\"]\n",
    "    output: [\"cat\", \"pig\"]\n",
    "    '''\n",
    "    return [labels[i] for i in np.argsort(lst)[::-1][:n]]  # np.argsort by default sorts values in ascending order\n",
    "\n",
    "def get_bottom_values(lst, n, labels):\n",
    "    '''\n",
    "    INPUT: LIST, INTEGER, LIST\n",
    "    OUTPUT: LIST\n",
    "\n",
    "    Given a list of values, find the indices with the lowest n values.\n",
    "    Return the labels for each of these indices.\n",
    "\n",
    "    e.g.\n",
    "    lst = [7, 3, 2, 4, 1]\n",
    "    n = 2\n",
    "    labels = [\"cat\", \"dog\", \"mouse\", \"pig\", \"rabbit\"]\n",
    "    output: [\"mouse\", \"rabbit\"]\n",
    "    '''\n",
    "    return [labels[i] for i in np.argsort(lst)[:n]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's use cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Draw an arbitrary review from test (unseen in training) documents\n",
    "query = np.random.choice(documents_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A really nice, relaxed setting, especially if you sit outside. We got there too early on a Sunday morning for the Bellagio fountains to be on, which was a shame, but it was still a pleasant place to be. Service was friendly and prompt. The food, overall, was very good. My 5 year old son got a yogurt parfait with cherry compote, which was perhaps too small for adult appetites, but suited him perfectly. My wife enjoyed her breakfast -- I forgot what it was, maybe Eggs Florentine. I was a little disappointed in what I ordered, which was the Egg White Omelette. It was a serious pile of egg whites, with a drizzle of \"sauce vert\" - a green sauce - on the top. Egg white overload. Really my fault, though, because the dish was as described and I should have made a better choice.\\n\\nWhen in Vegas, I\\'d definitely return to Mon Ami Gabi.'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform the drawn review(s) to vector(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_vectorized = vectorizer.transform([query]).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the similarity score(s) between vector(s) and training vectors\n",
    "cos_similarity = cosine_similarity(query_vectorized, vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 361026)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(361026,)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cos_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's find top 5 similar reviews\n",
    "n = 5\n",
    "top_5_reviews = get_top_values(cos_similarity, n, documents_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our search query:\n",
      "A really nice, relaxed setting, especially if you sit outside. We got there too early on a Sunday morning for the Bellagio fountains to be on, which was a shame, but it was still a pleasant place to be. Service was friendly and prompt. The food, overall, was very good. My 5 year old son got a yogurt parfait with cherry compote, which was perhaps too small for adult appetites, but suited him perfectly. My wife enjoyed her breakfast -- I forgot what it was, maybe Eggs Florentine. I was a little disappointed in what I ordered, which was the Egg White Omelette. It was a serious pile of egg whites, with a drizzle of \"sauce vert\" - a green sauce - on the top. Egg white overload. Really my fault, though, because the dish was as described and I should have made a better choice.\n",
      "\n",
      "When in Vegas, I'd definitely return to Mon Ami Gabi.\n"
     ]
    }
   ],
   "source": [
    "print('Our search query:')\n",
    "print(query) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most 5 similar reviews:\n",
      "[array([ \"McFries from McDonald's generally taste great, but this time :  Soggy, Cold, No Flavor, and pod was missing a scoop of fries. What happen to savory and crispy?\",\n",
      "       \"Worst experience everrrrrr!!! Waiting about 20 minutes for someone to approach us. Ryan the bartender was very rude. When we asked him about beer selections he was very short and not interested in helping us out. We are in the serving industry so we know how to tip, I understand when it's busy but is attitude was very unprofessional. We will not be back here for sure.\",\n",
      "       'Best spaghetti and meatballs i have ever had out to eat. Will eat here again. Sanwiches are great too. True italian.',\n",
      "       ...,\n",
      "       \"Breakfast was so delicious!  Best egg white omelette and pancake I've had in a very long time!  Service was friendly and quick too.\",\n",
      "       'Beautiful patio views of the Bellagio fountains..both daytime and nighttime. \\nGreat breakfast place! Egg white turkey omelet is a good healthy choice. I give this place !',\n",
      "       'Eggs Benedict no good for the price. If you like an under poached egg with runny egg white and uncooked yolk, then this is the place to go.'], dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "print('Most %s similar reviews:' % n)\n",
    "print(top_5_reviews)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: Does the result make sense to you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: The top two reviews confuses me cuz they are negative comments while our query is quite positive. The other similar reviews mentioned egg white, same as the query. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying positive/negative review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = vectors\n",
    "y = target_train\n",
    "y_test = target_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(361026, 2000)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(361026,)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive-Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a Naive-Bayes Classifier\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "\n",
    "model.fit(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80505005179682354"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for training set\n",
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80325220066440028"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for test set\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a Logistic Regression Classifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_lr = LogisticRegression()\n",
    "\n",
    "model_lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83165478386598191"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for training set\n",
    "model_lr.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82869718082287402"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for test set\n",
    "model_lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: What are the key features(words) that make the positive prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2000)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model_lr.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazing',\n",
       " 'best',\n",
       " 'awesome',\n",
       " 'phenomenal',\n",
       " 'incredible',\n",
       " 'thank',\n",
       " 'delicious',\n",
       " 'fantastic',\n",
       " 'perfect',\n",
       " 'heaven',\n",
       " 'perfection',\n",
       " 'impeccable',\n",
       " 'outstanding',\n",
       " 'excellent',\n",
       " 'favorite',\n",
       " 'highly',\n",
       " 'fabulous',\n",
       " 'great',\n",
       " 'beyond',\n",
       " 'perfectly']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's find it out by ranking\n",
    "n = 20\n",
    "get_top_values(model_lr.coef_[0], n, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_index = np.argsort(model.coef_)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2000)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1975,  845, 1466, ...,  124,  179,   68]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_index[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: It totally makes sense. Positive reviews are 5-star reviews in our case, thus top 20 words are all strongly positive comments, such as 'incredible', 'fantastic', 'phenomenal'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: What are the key features(words) that make the negative prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['worst',\n",
       " 'horrible',\n",
       " 'rude',\n",
       " 'terrible',\n",
       " 'disappointing',\n",
       " 'mediocre',\n",
       " 'bland',\n",
       " 'ok',\n",
       " 'lacked',\n",
       " 'awful',\n",
       " 'okay',\n",
       " 'poor',\n",
       " 'disgusting',\n",
       " 'worse',\n",
       " 'slow',\n",
       " 'lacking',\n",
       " 'meh',\n",
       " 'charged',\n",
       " 'average',\n",
       " 'overpriced']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's find it out by ranking\n",
    "n = 20\n",
    "get_bottom_values(model_lr.coef_[0], n, words) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: It also sounds reasonable to see the top rank 20 words for negative reviews (less than 5 stars) are those with negative sentiment such as 'worst', 'horrible', 'disappointing'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=3, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
       "            oob_score=False, random_state=1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a Random Forest Classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_rf = RandomForestClassifier(n_estimators = 200, max_depth = 30, min_samples_leaf = 3, random_state = 1)\n",
    "\n",
    "model_rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84139092475334187"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for training set\n",
    "model_rf.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79569690937528281"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for test set\n",
    "model_rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: What do you see from the training score and the test score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: Training score is clearly better than test score, which suggests overfitting. Should tune random forest further to reduce its complexity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: Can you tell what features (words) are important by inspecting the RFC model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazing',\n",
       " 'best',\n",
       " 'great',\n",
       " 'delicious',\n",
       " 'love',\n",
       " 'ok',\n",
       " 'awesome',\n",
       " 'bad',\n",
       " 'vegas',\n",
       " 'definitely',\n",
       " 'worst',\n",
       " 'good',\n",
       " 'would',\n",
       " 'pretty',\n",
       " 'favorite',\n",
       " 'horrible',\n",
       " 'highly',\n",
       " 'minutes',\n",
       " 'perfect',\n",
       " 'terrible']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_values(model_rf.feature_importances_, n, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit #1: Use cross validation to evaluate your classifiers\n",
    "\n",
    "[sklearn cross validation](http://scikit-learn.org/stable/modules/cross_validation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To be implemented\n",
    "from sklearn.model_selection import cross_val_score\n",
    "score = cross_val_score(model_lr, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.82815832,  0.82790665,  0.82689564,  0.82947164,  0.82925005])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8283364637952364"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit #2: Use grid search to find best predictable classifier\n",
    "\n",
    "\n",
    "[sklearn grid search tutorial (with cross validation)](http://scikit-learn.org/stable/modules/grid_search.html#grid-search)\n",
    "\n",
    "[sklearn grid search documentation (with cross validation)](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# laptop crushed, had to interrupt kernel...\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, accuracy_score\n",
    "\n",
    "# clf = RandomForestClassifier()\n",
    "\n",
    "# param_grid = {'n_estimators': [100, 200],\n",
    "#             'max_depth': [10, 15, 20],\n",
    "#              'min_samples_leaf': [5, 10],\n",
    "#              'n_jobs':[-1]\n",
    "#              }\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "param_grid = {'penalty': ['l1', 'l2'],\n",
    "            'C': [0.1, 1, 10]\n",
    "             }\n",
    "\n",
    "acc_scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "model_gs = GridSearchCV(clf, param_grid, cv=5, scoring=acc_scorer)\n",
    "\n",
    "model_gs = model_gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select best combination of parameters, named it model_best.\n",
    "model_best = model_gs.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for logistic regression on test set: \n",
      "\n",
      "    Previous: 0.828697\n",
      "    Grid Search: 0.833919\n"
     ]
    }
   ],
   "source": [
    "print('Score for logistic regression on test set: \\n')\n",
    "print('    Previous: %.6f' % (model_lr.score(X_test, y_test)))\n",
    "print('    Grid Search: %.6f' % (model_best.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
